---
title: "PDF extraction routine"
author: "Ron Hall"
format: html
editor: visual
---

```{r}
# Load packages 
library(pdftools)
library(tesseract)
library(dplyr)
library(tidyverse)
```

## PDF Extration Routine

This file was created as a demonstration of the routine that was used in extracting data from pdf files in order to perform QA/QC on existing data. In this process, we wanted to compare the invoice numbers, the item, and the dollar amount on each invoice to information that was delivered to us in a separate csv file.

-   For this process, the invoice pdf files were delivered by batch.

-   There were approximately 50 batch folders with approximately 2,000 pdf files in each folder.

    -   Due to the magnitude of this task, the code was developed, and then set up to run on a remote desktop that was specifically designated for the process.

-   Each of the pdf files contained all of the invoice information, but there were several different layouts. This also added some complexity to the problem since a single template could not be used.

-   The pdf files were a combination of image files as well as readable (editable text) files.

-   The code was developed as a nested for loop so that each file from each folder was examined.

    -   The first loop was set up to look at each folder.

    -   The nested loop looked at each of the files in the folder.

    -   The first part of the nested loop checked each file to determine if the file was a readable pdf.

        -   When the file was a readable pdf, the pdftools package was used.

        -   When the file was saved as an image, the tesseract package was used.

Using the functions from these packages, the data from each file was extracted and compiled into a single data set. The following code is an example of this process.

```{r}
# Create list of the names of the folders within the directory
my_dirs <- "Data/PDF Files"
folders <- list.files(my_dirs, full.names = TRUE)
folders

# Create an empty data frame to store the data
dat <- data.frame()

for(i in 1:length(folders)){
  
  # Create a list of the pdf files in the folder
  files <- list.files(folders[i], pattern = "\\.pdf", full.names = TRUE)
  
  # Read in each of the pdf files individually.
  for(j in 1:length(files)){
  
    # First, read the file in as text
    txt_dat <- pdf_text(files[j])
    
    # Use the teseract function for the cases identified as not having text
    if(length(txt_dat) == 1 & txt_dat[1] == ""){
      
      # Render pdf to png image
      img_file <- pdftools::pdf_convert(files[j], format = 'tiff', pages = 1, dpi = 400)

      # Extract text from png image
      txt_dat <- ocr(img_file)
    }
  
  # Parse the data into rows  
  rows <- scan(textConnection(txt_dat), 
           what="character", sep = "\n")
  
  # Append the rows to the dat data frame
  dat <- append(dat, rows)
  }
}
```

Once the list of all of the data rows was created, the data was then cleaned. The first step in the cleaning process included:

-   Create a data frame from the list of values.

-   The grepl function was then used to identify lines where the key values that we were looking for existed.

The following code is an example of the process used to identify the key values.

```{r}

# Create an empty data set
dat_1 <- data.frame()

# Loop through each row of the list, and extract the information from the list of values.
for (i in 1:length(dat)){ # i = 1
  row = unlist(dat[i])
  
  # add the information to a data frame
  dat_1 <- as.data.frame(rbind(dat_1, row))
}

# Rename the value column
names(dat_1) <- "value"

# Identify the lines that contain the key values. Note that this code has been modified to line up with the information contained in the sample files.
dat_1 <- dat_1 %>%
  mutate(flag = case_when(grepl("cash", value) ~ 1,
                            grepl( "payments", value) ~ 1))
```

We then output the data from each batch into a csv file, and all of the lines were reviewed to ensure that the information that was identified from each invoice was accurate, and the information was complete.

Once the data was reviewed, we then brought the data back into R. The data was then formatted, and the key value information was extracted into a single data set, and the noise was removed. Finally, we were able to compare it to the information that was provided to us in the csv file.
